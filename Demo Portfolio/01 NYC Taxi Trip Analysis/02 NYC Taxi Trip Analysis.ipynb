{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad4e2d7-cb4c-4022-a87a-95ed55ed518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (41169720, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_vendor_id</th>\n",
       "      <th>trip_start_datetime</th>\n",
       "      <th>trip_end_datetime</th>\n",
       "      <th>number_of_passengers</th>\n",
       "      <th>trip_distance_miles</th>\n",
       "      <th>fare_rate_code_id</th>\n",
       "      <th>stored_and_forwarded_flag</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>dropoff_location_id</th>\n",
       "      <th>payment_method_code</th>\n",
       "      <th>base_fare_amount</th>\n",
       "      <th>additional_surcharges_amount</th>\n",
       "      <th>mta_tax_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge_amount</th>\n",
       "      <th>total_trip_amount</th>\n",
       "      <th>congestion_surcharge_amount</th>\n",
       "      <th>airport_fee_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:57:55</td>\n",
       "      <td>2024-01-01 01:17:43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>186</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.70</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:03:00</td>\n",
       "      <td>2024-01-01 00:09:36</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:17:06</td>\n",
       "      <td>2024-01-01 00:35:01</td>\n",
       "      <td>1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>23.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:36:38</td>\n",
       "      <td>2024-01-01 00:44:56</td>\n",
       "      <td>1</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>79</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:46:51</td>\n",
       "      <td>2024-01-01 00:52:57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>211</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.10</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_vendor_id trip_start_datetime   trip_end_datetime  \\\n",
       "0               2 2024-01-01 00:57:55 2024-01-01 01:17:43   \n",
       "1               1 2024-01-01 00:03:00 2024-01-01 00:09:36   \n",
       "2               1 2024-01-01 00:17:06 2024-01-01 00:35:01   \n",
       "3               1 2024-01-01 00:36:38 2024-01-01 00:44:56   \n",
       "4               1 2024-01-01 00:46:51 2024-01-01 00:52:57   \n",
       "\n",
       "   number_of_passengers  trip_distance_miles  fare_rate_code_id  \\\n",
       "0                     1                 1.72                  1   \n",
       "1                     1                 1.80                  1   \n",
       "2                     1                 4.70                  1   \n",
       "3                     1                 1.40                  1   \n",
       "4                     1                 0.80                  1   \n",
       "\n",
       "  stored_and_forwarded_flag  pickup_location_id  dropoff_location_id  \\\n",
       "0                         N                 186                   79   \n",
       "1                         N                 140                  236   \n",
       "2                         N                 236                   79   \n",
       "3                         N                  79                  211   \n",
       "4                         N                 211                  148   \n",
       "\n",
       "   payment_method_code  base_fare_amount  additional_surcharges_amount  \\\n",
       "0                    2              17.7                           1.0   \n",
       "1                    1              10.0                           3.5   \n",
       "2                    1              23.3                           3.5   \n",
       "3                    1              10.0                           3.5   \n",
       "4                    1               7.9                           3.5   \n",
       "\n",
       "   mta_tax_amount  tip_amount  tolls_amount  improvement_surcharge_amount  \\\n",
       "0             0.5        0.00           0.0                           1.0   \n",
       "1             0.5        3.75           0.0                           1.0   \n",
       "2             0.5        3.00           0.0                           1.0   \n",
       "3             0.5        2.00           0.0                           1.0   \n",
       "4             0.5        3.20           0.0                           1.0   \n",
       "\n",
       "   total_trip_amount  congestion_surcharge_amount  airport_fee_amount  \n",
       "0              22.70                          2.5                 0.0  \n",
       "1              18.75                          2.5                 0.0  \n",
       "2              31.30                          2.5                 0.0  \n",
       "3              17.00                          2.5                 0.0  \n",
       "4              16.10                          2.5                 0.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "data_path = \"Data\"\n",
    "\n",
    "# Assistant\n",
    "# Find all parquet files matching the pattern for 2024 yellow taxi data and sort them\n",
    "files = sorted(glob.glob(os.path.join(data_path, \"yellow_tripdata_2024-*.parquet\")))\n",
    "\n",
    "# Read the first file to establish the column structure\n",
    "# This ensures consistent columns across all files\n",
    "df_first = pd.read_parquet(files[0])\n",
    "columns = df_first.columns\n",
    "\n",
    "# Store the first dataframe in our list of dataframes to concatenate\n",
    "dataframes = [df_first]\n",
    "\n",
    "# Process each remaining file, ensuring column consistency\n",
    "for file in files[1:]:\n",
    "    df = pd.read_parquet(file)\n",
    "    \n",
    "    # Align columns: add missing columns with NaN values, drop unexpected columns\n",
    "    # This ensures all dataframes have identical structure before concatenation\n",
    "    df = df.reindex(columns=columns)\n",
    "    \n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all monthly dataframes into one large dataframe\n",
    "# ignore_index=True creates a new sequential index rather than keeping original indices\n",
    "df_all = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Define a mapping dictionary to rename columns to more descriptive and consistent names\n",
    "# This improves readability and follows better naming conventions\n",
    "column_rename_map = {\n",
    "    \"VendorID\": \"data_vendor_id\",\n",
    "    \"tpep_pickup_datetime\": \"trip_start_datetime\",\n",
    "    \"tpep_dropoff_datetime\": \"trip_end_datetime\",\n",
    "    \"passenger_count\": \"number_of_passengers\",\n",
    "    \"trip_distance\": \"trip_distance_miles\",\n",
    "    \"RatecodeID\": \"fare_rate_code_id\",\n",
    "    \"store_and_fwd_flag\": \"stored_and_forwarded_flag\",\n",
    "    \"PULocationID\": \"pickup_location_id\",\n",
    "    \"DOLocationID\": \"dropoff_location_id\",\n",
    "    \"payment_type\": \"payment_method_code\",\n",
    "    \"fare_amount\": \"base_fare_amount\",\n",
    "    \"extra\": \"additional_surcharges_amount\",\n",
    "    \"mta_tax\": \"mta_tax_amount\",\n",
    "    \"tip_amount\": \"tip_amount\",\n",
    "    \"tolls_amount\": \"tolls_amount\",\n",
    "    \"improvement_surcharge\": \"improvement_surcharge_amount\",\n",
    "    \"congestion_surcharge\": \"congestion_surcharge_amount\",\n",
    "    \"Airport_fee\": \"airport_fee_amount\",\n",
    "    \"total_amount\": \"total_trip_amount\"\n",
    "}\n",
    "\n",
    "# Apply the column renaming to the dataframe\n",
    "df_all = df_all.rename(columns=column_rename_map)\n",
    "\n",
    "# Convert specific columns to nullable integer types (Int64)\n",
    "# This handles potential NaN values better than standard int types\n",
    "df_all[\"number_of_passengers\"] = df_all[\"number_of_passengers\"].astype(\"Int64\")\n",
    "df_all[\"fare_rate_code_id\"] = df_all[\"fare_rate_code_id\"].astype(\"Int64\")\n",
    "df_all[\"payment_method_code\"] = df_all[\"payment_method_code\"].astype(\"Int64\")\n",
    "\n",
    "print(\"Combined shape:\", df_all.shape)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5812b8fd-21e9-4a26-a6b1-461068f7ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample saved. Shape: (1000, 19)\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Create a random sample of 1000 rows from the full dataset\n",
    "# Using random_state=42 ensures reproducibility of the sample\n",
    "df_sample = df_all.sample(n=1000, random_state=42)\n",
    "\n",
    "# Export the sampled data to a CSV file in the specified data directory\n",
    "# index=False prevents writing row indices to the output file\n",
    "df_sample.to_csv(os.path.join(data_path, \"yellow_tripdata_sample_1000.csv\"), index=False)\n",
    "\n",
    "# Print confirmation message and display the dimensions of the sample dataset\n",
    "print(\"Sample saved. Shape:\", df_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d76cd1-6e90-4630-83ff-2a156041ca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41169720\n",
      "Null rows: 0\n",
      "Data type: int32 \n",
      "\n",
      "data_vendor_id\n",
      "2    31451503\n",
      "1     9715918\n",
      "6        2069\n",
      "7         230\n",
      " ame: count, dtype: int64\n",
      "\n",
      "data_vendor_id\n",
      "6    2069\n",
      "7     230\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Invalid rows: 2299 (0.0056% of dataset) \n",
      "\n",
      "Vendor ID-s after cleaning: \n",
      "\n",
      "data_vendor_id\n",
      "2    31451503\n",
      "1     9715918\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Define the column to analyze\n",
    "col = \"data_vendor_id\"\n",
    "\n",
    "# Print basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Display value counts including null values\n",
    "print(df_all[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "# Define the set of expected/valid vendor IDs\n",
    "EXPECTED_VENDOR_IDS = {1, 2}\n",
    "\n",
    "# Create a boolean mask to identify invalid vendor IDs\n",
    "# (rows that are not null AND not in our expected set)\n",
    "invalid_mask = (\n",
    "    df_all[col].notna()\n",
    "    & ~df_all[col].isin(EXPECTED_VENDOR_IDS)\n",
    ")\n",
    "\n",
    "# Display the counts of each invalid vendor ID\n",
    "print(df_all.loc[invalid_mask, col].value_counts(), \"\\n\")\n",
    "\n",
    "# Calculate statistics about invalid rows\n",
    "invalid_count = invalid_mask.sum()\n",
    "total_count = len(df_all)\n",
    "\n",
    "# Print the number and percentage of invalid rows\n",
    "print(\n",
    "    f\"Invalid rows: {invalid_count} \"\n",
    "    f\"({invalid_count / total_count:.4%} of dataset)\", \"\\n\"\n",
    ")\n",
    "\n",
    "# Remove rows with invalid vendor IDs from the dataframe\n",
    "invalid_idx = df_all.index[invalid_mask]\n",
    "df_all.drop(index=invalid_idx, inplace=True)\n",
    "\n",
    "# Verify the cleaning by displaying the updated value counts\n",
    "print(\"Vendor ID-s after cleaning:\", '\\n')\n",
    "print(df_all[col].value_counts(dropna=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a46aea1-5f9b-440b-bb67-53190e19cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41167421\n",
      "Null rows: 0\n",
      "Data type: datetime64[us] \n",
      "\n",
      "Value distribution summary:\n",
      "count                      41167421\n",
      "mean     2024-07-06 10:00:42.755670\n",
      "min             2002-12-31 16:46:07\n",
      "25%             2024-04-06 20:09:50\n",
      "50%             2024-07-03 23:37:02\n",
      "75%             2024-10-08 17:31:40\n",
      "max             2026-06-26 23:53:12\n",
      "Name: trip_start_datetime, dtype: object \n",
      "\n",
      "Rows with invalid values before cleaning: 56\n",
      "Rows with invalid values after cleaning: 0\n",
      "Null rows after cleaning: 0\n",
      "Value distribution summary after cleaning:\n",
      "count                      41167365\n",
      "mean     2024-07-06 10:09:15.688188\n",
      "min             2024-01-01 00:00:00\n",
      "25%             2024-04-06 20:10:05\n",
      "50%             2024-07-03 23:37:37\n",
      "75%             2024-10-08 17:31:44\n",
      "max             2024-12-31 23:59:58\n",
      "Name: trip_start_datetime, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "col = \"trip_start_datetime\"\n",
    "\n",
    "# Basic diagnostics\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Display statistical summary of the datetime column\n",
    "print(\"Value distribution summary:\")\n",
    "print(df_all[col].describe(), \"\\n\")\n",
    "\n",
    "# Define valid date range boundaries for trip start times\n",
    "# - We expect all valid trips to start in 2024\n",
    "# - Any trips outside this range are considered invalid\n",
    "MIN_DATE = pd.Timestamp(\"2024-01-01 00:00:00\")\n",
    "MAX_DATE = pd.Timestamp(\"2025-01-01 00:00:00\")\n",
    "\n",
    "# Helper function to identify invalid datetime values\n",
    "# Returns a boolean mask where True indicates an invalid value\n",
    "def get_invalid_mask(df, column, min_value, max_value):\n",
    "    return df[column].notna() & ((df[column] < min_value) | (df[column] >= max_value))\n",
    "\n",
    "# Identify rows with invalid trip start times\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_DATE, MAX_DATE)\n",
    "print(\"Rows with invalid values before cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Remove rows with invalid trip start times\n",
    "invalid_idx = df_all.index[invalid_mask]\n",
    "df_all.drop(index=invalid_idx, inplace=True)\n",
    "\n",
    "# Verify that all invalid values have been removed\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_DATE, MAX_DATE)\n",
    "print(\"Rows with invalid values after cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Final data quality check after cleaning\n",
    "print(\"Null rows after cleaning:\", df_all[col].isna().sum())\n",
    "print(\"Value distribution summary after cleaning:\")\n",
    "print(df_all[col].describe(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b368f48-b82d-4097-ac8a-f52ae233b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41167365\n",
      "Null rows: 0\n",
      "Data type: datetime64[us] \n",
      "\n",
      "Value distribution summary:\n",
      "count                      41167365\n",
      "mean     2024-07-06 10:26:43.677426\n",
      "min             2024-01-01 00:02:42\n",
      "25%             2024-04-06 20:26:12\n",
      "50%             2024-07-03 23:53:19\n",
      "75%             2024-10-08 17:51:46\n",
      "max             2025-01-01 22:59:33\n",
      "Name: trip_end_datetime, dtype: object \n",
      "\n",
      "Rows with invalid values before cleaning: 611\n",
      "Rows with invalid values after cleaning: 0\n",
      "Null rows after cleaning: 0\n",
      "Value distribution summary after cleaning:\n",
      "count                      41166754\n",
      "mean     2024-07-06 10:22:54.632399\n",
      "min             2024-01-01 00:02:42\n",
      "25%             2024-04-06 20:25:04\n",
      "50%      2024-07-03 23:46:13.500000\n",
      "75%             2024-10-08 17:48:35\n",
      "max             2024-12-31 23:59:59\n",
      "Name: trip_end_datetime, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "col = \"trip_end_datetime\"\n",
    "\n",
    "# Basic diagnostics\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Display statistical summary of the datetime column\n",
    "print(\"Value distribution summary:\")\n",
    "print(df_all[col].describe(), \"\\n\")\n",
    "\n",
    "# Define valid date range boundaries for trip start times\n",
    "# - We expect all valid trips to start in 2024\n",
    "# - Any trips outside this range are considered invalid\n",
    "MIN_DATE = pd.Timestamp(\"2024-01-01 00:00:00\")\n",
    "MAX_DATE = pd.Timestamp(\"2025-01-01 00:00:00\")\n",
    "\n",
    "# Helper function to identify invalid datetime values\n",
    "# Returns a boolean mask where True indicates an invalid value\n",
    "def get_invalid_mask(df, column, min_value, max_value):\n",
    "    return df[column].notna() & ((df[column] < min_value) | (df[column] >= max_value))\n",
    "\n",
    "# Identify rows with invalid trip start times\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_DATE, MAX_DATE)\n",
    "print(\"Rows with invalid values before cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Remove rows with invalid trip start times\n",
    "invalid_idx = df_all.index[invalid_mask]\n",
    "df_all.drop(index=invalid_idx, inplace=True)\n",
    "\n",
    "# Verify that all invalid values have been removed\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_DATE, MAX_DATE)\n",
    "print(\"Rows with invalid values after cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Final data quality check after cleaning\n",
    "print(\"Null rows after cleaning:\", df_all[col].isna().sum())\n",
    "print(\"Value distribution summary after cleaning:\")\n",
    "print(df_all[col].describe(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c7c493-0305-4bcf-b4af-9335b4bfbb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41166754\n",
      "Null rows: 4089128\n",
      "Data type: Int64 \n",
      "\n",
      "Value distribution (including nulls):\n",
      "number_of_passengers\n",
      "0         401351\n",
      "1       28632114\n",
      "2        5410593\n",
      "3        1282030\n",
      "4         814855\n",
      "5         320605\n",
      "6         215794\n",
      "7             56\n",
      "8            192\n",
      "9             36\n",
      "<NA>     4089128\n",
      "Name: count, dtype: Int64 \n",
      "\n",
      "Rows with zero passengers before cleaning: 401351 \n",
      "\n",
      "Rows with zero passengers after cleaning: 0 \n",
      "\n",
      "Value distribution after cleaning (including nulls):\n",
      "number_of_passengers\n",
      "1       28632114\n",
      "2        5410593\n",
      "3        1282030\n",
      "4         814855\n",
      "5         320605\n",
      "6         215794\n",
      "7             56\n",
      "8            192\n",
      "9             36\n",
      "<NA>     4490479\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Define the column to analyze\n",
    "col = \"number_of_passengers\"\n",
    "\n",
    "# Display basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Show the distribution of values including null values\n",
    "print(\"Value distribution (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index(), \"\\n\")\n",
    "\n",
    "# Create a mask to identify rows where passenger count is zero\n",
    "zero_mask = df_all[col] == 0\n",
    "print(\"Rows with zero passengers before cleaning:\", zero_mask.sum(), '\\n')\n",
    "\n",
    "# Replace zero passenger counts with NA values (data cleaning)\n",
    "df_all.loc[zero_mask, col] = pd.NA\n",
    "\n",
    "# Verify that zero values were successfully replaced\n",
    "zero_mask = df_all[col] == 0\n",
    "print(\"Rows with zero passengers after cleaning:\", zero_mask.sum(), '\\n')\n",
    "\n",
    "# Display the updated value distribution after cleaning\n",
    "print(\"Value distribution after cleaning (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7d9e48-a2b4-4f18-9f37-7fb8e15d362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41166754\n",
      "Null rows: 4089128\n",
      "Data type: Int64 \n",
      "\n",
      "Value distribution (including nulls):\n",
      "fare_rate_code_id\n",
      "1       34650263\n",
      "2        1406796\n",
      "3         129950\n",
      "4         101630\n",
      "5         321937\n",
      "6             76\n",
      "99        466974\n",
      "<NA>     4089128\n",
      " ame: count, dtype: Int64\n",
      "\n",
      "Rows with invalid codes before cleaning: 466974 \n",
      "\n",
      "Rows with invalid codes after cleaning: 0 \n",
      "\n",
      "Value distribution after cleaning (including nulls):\n",
      "fare_rate_code_id\n",
      "1       34650263\n",
      "2        1406796\n",
      "3         129950\n",
      "4         101630\n",
      "5         321937\n",
      "6             76\n",
      "<NA>     4556102\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "col = \"fare_rate_code_id\"\n",
    "\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "print(\"Value distribution (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index(), \"\\n\")\n",
    "\n",
    "# Valid TLC Rate Code IDs (Yellow Taxi standard):\n",
    "# 1 = Standard rate\n",
    "# 2 = JFK\n",
    "# 3 = Newark\n",
    "# 4 = Nassau or Westchester\n",
    "# 5 = Negotiated fare\n",
    "# 6 = Group ride\n",
    "EXPECTED_RATE_CODE_IDS = {1, 2, 3, 4, 5, 6}\n",
    "\n",
    "def get_invalid_mask(df, column, expected_values):\n",
    "    return df[column].notna() & ~df[column].isin(expected_values)\n",
    "\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_RATE_CODE_IDS)\n",
    "print(\"Rows with invalid codes before cleaning:\", invalid_mask.sum(), '\\n')\n",
    "\n",
    "df_all.loc[invalid_mask, col] = pd.NA\n",
    "\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_RATE_CODE_IDS)\n",
    "print(\"Rows with invalid codes after cleaning:\", invalid_mask.sum(), '\\n')\n",
    "\n",
    "print(\"Value distribution after cleaning (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4402e773-3d92-4dd4-9c57-f2d06db465c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41166754\n",
      "Null rows: 4556102\n",
      "Data type: Int64 \n",
      "\n",
      "Value distribution (including nulls):\n",
      "fare_rate_code_id\n",
      "1       34650263\n",
      "2        1406796\n",
      "3         129950\n",
      "4         101630\n",
      "5         321937\n",
      "6             76\n",
      "<NA>     4556102\n",
      " ame: count, dtype: Int64\n",
      "\n",
      "Rows with invalid codes before cleaning: 0 \n",
      "\n",
      "Rows with invalid codes after cleaning: 0 \n",
      "\n",
      "Value distribution after cleaning (including nulls):\n",
      "fare_rate_code_id\n",
      "1       34650263\n",
      "2        1406796\n",
      "3         129950\n",
      "4         101630\n",
      "5         321937\n",
      "6             76\n",
      "<NA>     4556102\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "col = \"fare_rate_code_id\"\n",
    "\n",
    "# Display basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Show the distribution of values in the column, including null values\n",
    "print(\"Value distribution (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index(), \"\\n\")\n",
    "\n",
    "# Define the valid rate code IDs according to TLC Yellow Taxi standards\n",
    "# 1 = Standard rate\n",
    "# 2 = JFK\n",
    "# 3 = Newark\n",
    "# 4 = Nassau or Westchester\n",
    "# 5 = Negotiated fare\n",
    "# 6 = Group ride\n",
    "EXPECTED_RATE_CODE_IDS = {1, 2, 3, 4, 5, 6}\n",
    "\n",
    "# Helper function to identify rows with invalid values\n",
    "# Returns a boolean mask where True indicates an invalid value\n",
    "def get_invalid_mask(df, column, expected_values):\n",
    "    return df[column].notna() & ~df[column].isin(expected_values)\n",
    "\n",
    "# Identify rows with invalid rate codes\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_RATE_CODE_IDS)\n",
    "print(\"Rows with invalid codes before cleaning:\", invalid_mask.sum(), '\\n')\n",
    "\n",
    "# Replace invalid rate codes with NA values\n",
    "df_all.loc[invalid_mask, col] = pd.NA\n",
    "\n",
    "# Verify that all invalid values have been replaced\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_RATE_CODE_IDS)\n",
    "print(\"Rows with invalid codes after cleaning:\", invalid_mask.sum(), '\\n')\n",
    "\n",
    "# Display the updated distribution of values after cleaning\n",
    "print(\"Value distribution after cleaning (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca449b13-04ea-446a-bce7-b3769d1e42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41166754\n",
      "Null rows: 4089128\n",
      "Data type: object \n",
      "\n",
      "Value distribution (including nulls):\n",
      "stored_and_forwarded_flag\n",
      "N       36902456\n",
      "None     4089128\n",
      "Y         175170\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "col = \"stored_and_forwarded_flag\"\n",
    "\n",
    "# Display basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Show the distribution of values in the column, including null values\n",
    "print(\"Value distribution (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "# No cleaning needed as the column only contains expected values and nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef1c10ff-2f21-4da8-8972-e81579718912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41166754\n",
      "Null rows: 0\n",
      "Data type: int32 \n",
      "\n",
      "Value distribution (including nulls):\n",
      "pickup_location_id\n",
      "1        5940\n",
      "2          62\n",
      "3        1725\n",
      "4       71457\n",
      "5           7\n",
      "        ...  \n",
      "261    218052\n",
      "262    562784\n",
      "263    766730\n",
      "264    122197\n",
      "265     22751\n",
      "Name: count, Length: 263, dtype: int64 \n",
      "\n",
      "Rows with invalid codes before cleaning: 144948\n",
      "Rows with invalid codes after cleaning: 0\n",
      "Value distribution after cleaning (including nulls):\n",
      "pickup_location_id\n",
      "1        5940\n",
      "2          62\n",
      "3        1725\n",
      "4       71457\n",
      "5           7\n",
      "        ...  \n",
      "259      2222\n",
      "260     13154\n",
      "261    218052\n",
      "262    562784\n",
      "263    766730\n",
      "Name: count, Length: 261, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Define the column to analyze\n",
    "col = \"pickup_location_id\"\n",
    "\n",
    "# Display basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Show the distribution of values in the column (including null values)\n",
    "print(\"Value distribution (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index(), \"\\n\")\n",
    "\n",
    "# Define the expected range of location IDs (1 through 263)\n",
    "EXPECTED_LOCATION_IDS = set(range(1, 264))\n",
    "\n",
    "# Helper function to identify rows with invalid values\n",
    "def get_invalid_mask(df, column, expected_values):\n",
    "    # Returns a boolean mask where True indicates values that exist but are not in expected_values\n",
    "    return df[column].notna() & ~df[column].isin(expected_values)\n",
    "\n",
    "# Apply the function to identify invalid location IDs\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_LOCATION_IDS)\n",
    "print(\"Rows with invalid codes before cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Get indices of invalid rows and remove them from the dataframe\n",
    "invalid_idx = df_all.index[invalid_mask]\n",
    "df_all.drop(index=invalid_idx, inplace=True)\n",
    "\n",
    "# Verify that all invalid values have been removed\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_LOCATION_IDS)\n",
    "print(\"Rows with invalid codes after cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Display the updated distribution of values after cleaning\n",
    "print(\"Value distribution after cleaning (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d7290bc-84d4-4aeb-a983-a9ee46067a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41021806\n",
      "Null rows: 0\n",
      "Data type: int32 \n",
      "\n",
      "Value distribution (including nulls):\n",
      "dropoff_location_id\n",
      "1      110792\n",
      "2          81\n",
      "3        3643\n",
      "4      164351\n",
      "5         182\n",
      "        ...  \n",
      "261    209652\n",
      "262    612857\n",
      "263    832758\n",
      "264    114820\n",
      "265    164609\n",
      "Name: count, Length: 262, dtype: int64 \n",
      "\n",
      "Rows with invalid codes before cleaning: 279429\n",
      "Rows with invalid codes after cleaning: 0\n",
      "Value distribution after cleaning (including nulls):\n",
      "dropoff_location_id\n",
      "1      110792\n",
      "2          81\n",
      "3        3643\n",
      "4      164351\n",
      "5         182\n",
      "        ...  \n",
      "259      4999\n",
      "260     29378\n",
      "261    209652\n",
      "262    612857\n",
      "263    832758\n",
      "Name: count, Length: 260, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Define the column to analyze\n",
    "col = \"dropoff_location_id\"\n",
    "\n",
    "# Display basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Show the distribution of values in the column (including null values)\n",
    "print(\"Value distribution (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index(), \"\\n\")\n",
    "\n",
    "# Define the expected range of location IDs (1 through 263)\n",
    "EXPECTED_LOCATION_IDS = set(range(1, 264))\n",
    "\n",
    "# Helper function to identify rows with invalid values\n",
    "def get_invalid_mask(df, column, expected_values):\n",
    "    # Returns a boolean mask where True indicates values that exist but are not in expected_values\n",
    "    return df[column].notna() & ~df[column].isin(expected_values)\n",
    "\n",
    "# Apply the function to identify invalid location IDs\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_LOCATION_IDS)\n",
    "print(\"Rows with invalid codes before cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Get indices of invalid rows and remove them from the dataframe\n",
    "invalid_idx = df_all.index[invalid_mask]\n",
    "df_all.drop(index=invalid_idx, inplace=True)\n",
    "\n",
    "# Verify that all invalid values have been removed\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_LOCATION_IDS)\n",
    "print(\"Rows with invalid codes after cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Display the updated distribution of values after cleaning\n",
    "print(\"Value distribution after cleaning (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14fffab4-dd01-4113-88c2-a813da8ecc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 40742377\n",
      "Null rows: 0\n",
      "Data type: Int64 \n",
      "\n",
      "Value distribution (including nulls):\n",
      "payment_method_code\n",
      "0     4080646\n",
      "1    30137475\n",
      "2     5457626\n",
      "3      283726\n",
      "4      782904\n",
      "Name: count, dtype: Int64 \n",
      "\n",
      "Rows with invalid codes before cleaning: 4080646\n",
      "Rows with invalid codes after cleaning: 0\n",
      "Value distribution after cleaning (including nulls):\n",
      "payment_method_code\n",
      "1       30137475\n",
      "2        5457626\n",
      "3         283726\n",
      "4         782904\n",
      "<NA>     4080646\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Define the column to analyze\n",
    "col = \"payment_method_code\"\n",
    "\n",
    "# Display basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Show the distribution of values in the column, including null values\n",
    "print(\"Value distribution (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index(), \"\\n\")\n",
    "\n",
    "# Valid TLC payment type codes:\n",
    "# 1 = Credit card\n",
    "# 2 = Cash\n",
    "# 3 = No charge\n",
    "# 4 = Dispute\n",
    "# 5 = Unknown\n",
    "# 6 = Voided trip\n",
    "EXPECTED_PAYMENT_CODES = {1, 2, 3, 4, 5, 6}\n",
    "\n",
    "# Define a function to identify rows with invalid values\n",
    "def get_invalid_mask(df, column, expected_values):\n",
    "    \"\"\"\n",
    "    Creates a boolean mask identifying rows with values that are not null\n",
    "    and not in the expected values set.\n",
    "    \"\"\"\n",
    "    return df[column].notna() & ~df[column].isin(expected_values)\n",
    "\n",
    "# Identify rows with invalid payment codes\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_PAYMENT_CODES)\n",
    "print(\"Rows with invalid codes before cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Replace invalid payment codes with NA\n",
    "df_all.loc[invalid_mask, col] = pd.NA\n",
    "\n",
    "# Verify that all invalid codes have been removed\n",
    "invalid_mask = get_invalid_mask(df_all, col, EXPECTED_PAYMENT_CODES)\n",
    "print(\"Rows with invalid codes after cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Display the updated distribution of values\n",
    "print(\"Value distribution after cleaning (including nulls):\")\n",
    "print(df_all[col].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59811d05-8c55-4034-ad22-341ef931f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 40742377\n",
      "Null rows: 0\n",
      "Data type: float64 \n",
      "\n",
      "Value distribution summary:\n",
      "count    4.074238e+07\n",
      "mean     2.742164e+01\n",
      "std      7.749884e+01\n",
      "min     -2.265450e+03\n",
      "25%      1.572000e+01\n",
      "50%      2.100000e+01\n",
      "75%      3.024000e+01\n",
      "max      3.355509e+05\n",
      "Name: total_trip_amount, dtype: float64 \n",
      "\n",
      "Rows with invalid values before cleaning: 600606\n",
      "Rows with invalid values after cleaning: 0\n",
      "Null rows after cleaning: 600606\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Define the column to analyze\n",
    "col = \"total_trip_amount\"\n",
    "\n",
    "# Print basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Display statistical summary of the column values\n",
    "print(\"Value distribution summary:\")\n",
    "print(df_all[col].describe(), \"\\n\")\n",
    "\n",
    "# Define acceptable value range for the total trip amount\n",
    "MIN_TOTAL_AMOUNT = 0  # Minimum acceptable value (no negative trip amounts)\n",
    "MAX_REASONABLE_TOTAL = 1000  # Maximum reasonable value, used as a threshold for outliers\n",
    "\n",
    "# Helper function to identify invalid values based on specified range\n",
    "def get_invalid_mask(df, column, min_value, max_value):\n",
    "    \"\"\"\n",
    "    Creates a boolean mask identifying values outside the specified range.\n",
    "    \"\"\"\n",
    "    return df[column].notna() & ((df[column] < min_value) | (df[column] > max_value))\n",
    "\n",
    "# Check how many invalid values exist before cleaning\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_TOTAL_AMOUNT, MAX_REASONABLE_TOTAL)\n",
    "print(\"Rows with invalid values before cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Replace invalid values with NA\n",
    "df_all.loc[invalid_mask, col] = pd.NA\n",
    "\n",
    "# Verify that invalid values were successfully removed\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_TOTAL_AMOUNT, MAX_REASONABLE_TOTAL)\n",
    "print(\"Rows with invalid values after cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Report the final count of null values after cleaning\n",
    "print(\"Null rows after cleaning:\", df_all[col].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdfcf989-ef38-4cca-a6c4-06b166ac35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 40742377\n",
      "Null rows: 0\n",
      "Data type: float64 \n",
      "\n",
      "Value distribution summary:\n",
      "count    4.074238e+07\n",
      "mean     4.889713e+00\n",
      "std      4.177569e+02\n",
      "min      0.000000e+00\n",
      "25%      1.010000e+00\n",
      "50%      1.750000e+00\n",
      "75%      3.310000e+00\n",
      "max      3.986086e+05\n",
      "Name: trip_distance_miles, dtype: float64 \n",
      "\n",
      "Rows with invalid values before cleaning: 1461\n",
      "Rows with invalid values after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Define the column name we're analyzing\n",
    "col = \"trip_distance_miles\"\n",
    "\n",
    "# Print basic information about the column\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(\"Null rows:\", df_all[col].isna().sum())\n",
    "print(\"Data type:\", df_all[col].dtype, \"\\n\")\n",
    "\n",
    "# Display statistical summary of the column values\n",
    "print(\"Value distribution summary:\")\n",
    "print(df_all[col].describe(), \"\\n\")\n",
    "\n",
    "# Define reasonable boundaries for trip distance\n",
    "MIN_DISTANCE = 0  # Trips can't have negative distance\n",
    "MAX_REASONABLE_DISTANCE = 50  # Set upper limit for reasonable trip distance in miles\n",
    "\n",
    "# Helper function to identify invalid values based on min/max thresholds\n",
    "def get_invalid_mask(df, column, min_value, max_value):\n",
    "    return df[column].notna() & ((df[column] < min_value) | (df[column] > max_value))\n",
    "\n",
    "# Identify rows with invalid distance values\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_DISTANCE, MAX_REASONABLE_DISTANCE)\n",
    "print(\"Rows with invalid values before cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Get indices of invalid rows and remove them from the dataframe\n",
    "invalid_idx = df_all.index[invalid_mask]\n",
    "df_all.drop(index=invalid_idx, inplace=True)\n",
    "\n",
    "# Verify that all invalid values have been removed\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_DISTANCE, MAX_REASONABLE_DISTANCE)\n",
    "print(\"Rows with invalid values after cleaning:\", invalid_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea7aa67-8ec3-4a43-a7ea-3b750137e9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       trip_duration_seconds  trip_duration_minutes\n",
      "count           4.074092e+07           4.074092e+07\n",
      "mean            1.041749e+03           1.736248e+01\n",
      "std             2.036042e+03           3.393403e+01\n",
      "min            -8.562300e+04          -1.427050e+03\n",
      "25%             4.690000e+02           7.820000e+00\n",
      "50%             7.770000e+02           1.295000e+01\n",
      "75%             1.260000e+03           2.100000e+01\n",
      "max             5.860510e+05           9.767520e+03\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert string datetime columns to pandas datetime objects\n",
    "# errors=\"coerce\" will set invalid parsing as NaT (Not a Time)\n",
    "df_all[\"trip_start_datetime\"] = pd.to_datetime(df_all[\"trip_start_datetime\"], errors=\"coerce\")\n",
    "df_all[\"trip_end_datetime\"] = pd.to_datetime(df_all[\"trip_end_datetime\"], errors=\"coerce\")\n",
    "\n",
    "# Calculate trip duration by subtracting start time from end time\n",
    "# This creates a timedelta object\n",
    "df_all[\"trip_duration\"] = df_all[\"trip_end_datetime\"] - df_all[\"trip_start_datetime\"]\n",
    "\n",
    "# Convert trip duration to seconds\n",
    "df_all[\"trip_duration_seconds\"] = df_all[\"trip_duration\"].dt.total_seconds()\n",
    "# Convert seconds to minutes and round to 2 decimal places\n",
    "df_all[\"trip_duration_minutes\"] = (df_all[\"trip_duration_seconds\"] / 60.0).round(2)\n",
    "\n",
    "# Display statistical summary of the trip duration columns\n",
    "print(df_all[[\"trip_duration_seconds\", \"trip_duration_minutes\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4efee2-7943-4ab2-aa3f-14f46a15164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with invalid values before cleaning: 384397\n",
      "Rows with invalid values after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Define the column name for trip duration\n",
    "col = 'trip_duration_seconds'\n",
    "\n",
    "# Set minimum and maximum acceptable values for trip duration\n",
    "MIN_DURATION_SECONDS = 30\n",
    "MAX_REASONABLE_DURATION_SECONDS = 10800 # 3 hours\n",
    "\n",
    "# Function to create a boolean mask identifying invalid values in a dataframe column\n",
    "def get_invalid_mask(df, column, min_value, max_value):\n",
    "    return df[column].notna() & ((df[column] < min_value) | (df[column] > max_value))\n",
    "\n",
    "# Create a mask to identify rows with invalid trip durations\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_DURATION_SECONDS, MAX_REASONABLE_DURATION_SECONDS)\n",
    "print(\"Rows with invalid values before cleaning:\", invalid_mask.sum())\n",
    "\n",
    "# Get indices of invalid rows and remove them from the dataframe\n",
    "invalid_idx = df_all.index[invalid_mask]\n",
    "df_all.drop(index=invalid_idx, inplace=True)\n",
    "\n",
    "# Verify that all invalid values have been removed\n",
    "invalid_mask = get_invalid_mask(df_all, col, MIN_DURATION_SECONDS, MAX_REASONABLE_DURATION_SECONDS)\n",
    "print(\"Rows with invalid values after cleaning:\", invalid_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8d091a7-0b91-4afe-a01c-a6abac5c2f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows 2\n",
      "Duplicated rows deleted\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Check how many duplicate rows exist in the dataframe\n",
    "print(\"Number of duplicated rows\", df_all.duplicated().sum())\n",
    "# Remove all duplicate rows from the dataframe (modifying it in place)\n",
    "df_all.drop_duplicates(inplace=True)\n",
    "# Confirm to the user that duplicates have been removed\n",
    "print(\"Duplicated rows deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c784ba1-3919-423f-9ec4-e5367cac4b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: (40356517, 13)\n",
      "Cleaned output saved. Shape: (40356517, 13)\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "output_columns = [\n",
    "    # --- Identifiers / time ---\n",
    "    \"trip_start_datetime\",\n",
    "    \"trip_end_datetime\",\n",
    "    \"trip_duration_seconds\",\n",
    "    \"trip_duration_minutes\",\n",
    "\n",
    "    # --- Cleaned categorical fields ---\n",
    "    \"data_vendor_id\",\n",
    "    \"fare_rate_code_id\",\n",
    "    \"stored_and_forwarded_flag\",\n",
    "    \"payment_method_code\",\n",
    "\n",
    "    # --- Cleaned numeric fields ---\n",
    "    \"number_of_passengers\",\n",
    "    \"trip_distance_miles\",\n",
    "    \"total_trip_amount\",\n",
    "\n",
    "    # --- Location IDs (validated) ---\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "]\n",
    "\n",
    "# Create a new dataframe with only the selected columns\n",
    "df_output = df_all[output_columns].copy()\n",
    "\n",
    "# Display the dimensions of the final cleaned dataset\n",
    "print(\"Final output shape:\", df_output.shape)\n",
    "\n",
    "# Save the cleaned dataframe to a Parquet file for efficient storage\n",
    "df_output.to_parquet(os.path.join(data_path, \"yellow_tripdata_cleand.parquet\"), index=False)\n",
    "# Confirm the save operation and display the shape again\n",
    "print(\"Cleaned output saved. Shape:\", df_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94b2b10-c7a9-4151-9417-75f8c7fe4398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample saved. Shape: (1000, 13)\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# Create a random sample of 1000 rows from the cleaned dataset\n",
    "# Using random_state=42 ensures reproducibility of the sampling\n",
    "df_sample = df_output.sample(n=1000, random_state=42)\n",
    "\n",
    "# Write the sample to a CSV file in the specified data path\n",
    "# index=False prevents writing row indices to the CSV file\n",
    "df_sample.to_csv(os.path.join(data_path, \"yellow_tripdata_cleand_sample_1000.csv\"), index=False)\n",
    "\n",
    "# Print confirmation message and display the dimensions of the sample dataframe\n",
    "print(\"Sample saved. Shape:\", df_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12e6d7bb-da9b-477c-a560-447a202693d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process further from memory.\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the path to the data directory\n",
    "data_path = \"Data\"\n",
    "\n",
    "try:\n",
    "    # Check if df_output already exists in memory\n",
    "    df_output\n",
    "    print(\"Process further from memory.\")\n",
    "except NameError:\n",
    "    # If df_output doesn't exist, load it from the parquet file\n",
    "    df_output = pd.read_parquet(os.path.join(data_path, \"yellow_tripdata_cleaned.parquet\"))\n",
    "    print(\"Process further from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2e5fe-2bd2-4063-abeb-863f9d831ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant\n",
    "# Create random sample for Tableau\n",
    "import pandas as pd\n",
    "\n",
    "# Sample 5% of the original dataframe with a fixed random seed for reproducibility\n",
    "sampled_df = (df_output.sample(frac=0.05, random_state=42))\n",
    "\n",
    "# Reset index (nice for Tableau extracts)\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n",
    "\n",
    "# Print statistics about the sampling\n",
    "print(\"Sampled rows:\", len(sampled_df))  # Display the number of rows in the sample\n",
    "print(\"Sampling fraction:\", len(sampled_df) / len(df_output))  # Calculate and display the actual sampling ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a42ee-49dc-40b2-99da-4c6905a38c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant\n",
    "# Define the output file path for the cleaned dataset\n",
    "output_path = r\"Data\\tableau_trips_cleaned.csv\"\n",
    "\n",
    "# Save the sampled dataframe to a CSV file without including the index\n",
    "sampled_df.to_csv(\n",
    "    output_path,\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Print confirmation message showing where the file was saved\n",
    "print(f\"Saved Tableau dataset to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fbf98a-3aa2-4804-a0f0-83f17aa7ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884da47a-1f28-4581-beb4-bc651036d910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
