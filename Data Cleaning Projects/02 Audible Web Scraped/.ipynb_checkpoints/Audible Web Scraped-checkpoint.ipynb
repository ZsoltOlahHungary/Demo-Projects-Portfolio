{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43480823-9c3a-4908-8f6a-5d98cad5da17",
   "metadata": {},
   "source": [
    "This dataset documents the development of the audiobook market from 1998 through 2025, including planned releases. It contains information such as authors and release dates, providing a structured overview of key details across this period.\n",
    "The dataset is designed to capture both foundational aspects and historical trends in audiobooks, with the intention of being expanded and updated with additional details over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "891f8559-fe52-4dd4-917b-5898679736e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7cb1f3c8-3c4c-4f26-a83a-5b0829a7ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "def update_google_sheet(df, sheet_name, worksheet_name=\"Working Dataset\", creds_file=\"credentials.json\"):\n",
    "    \"\"\"\n",
    "    Update a Google Sheet with the contents of a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset you want to upload.\n",
    "    sheet_name : str\n",
    "        The name of the Google Sheet (must already exist).\n",
    "    worksheet_name : str, optional\n",
    "        The name of the worksheet/tab inside the sheet (default: \"Sheet1\").\n",
    "    creds_file : str, optional\n",
    "        Path to your Google service account JSON credentials file (default: \"credentials.json\").\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Define scope\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "             \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "    # Load credentials\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(creds_file, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    # Open the sheet and worksheet\n",
    "    sheet = client.open(sheet_name).worksheet(worksheet_name)\n",
    "\n",
    "    # Clear existing content\n",
    "    sheet.clear()\n",
    "\n",
    "    # Update with new data\n",
    "    sheet.update([df.columns.values.tolist()] + df.values.tolist())\n",
    "\n",
    "    print(f\"✅ Google Sheet '{sheet_name}' → '{worksheet_name}' updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b165c8e4-5a0c-45e1-b6e6-f25ff78fdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with explicit UTF-8 handling\n",
    "df = pd.read_csv(\"audible_uncleaned.csv\", encoding=\"utf-8\", encoding_errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "78675ace-4668-4396-8ed2-28d5f215ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure UTF-8 consistency and clean whitespace in 'name' column\n",
    "df[\"name_normalized\"] = (\n",
    "    df[\"name\"]\n",
    "    .astype(str)                              # Ensure all values are strings\n",
    "    .str.encode(\"utf-8\", errors=\"ignore\")     # Enforce UTF-8 encoding\n",
    "    .str.decode(\"utf-8\")\n",
    "    .str.strip()                              # Trim leading/trailing whitespace\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)    # Collapse multiple spaces\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fafd4f94-d422-410e-9157-776dcf5e0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_normalize_authors(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return pd.NA\n",
    "\n",
    "    s = str(text)\n",
    "\n",
    "    # 1) Remove constant prefix \"Writtenby:\"\n",
    "    s = re.sub(r\"^\\s*Writtenby:\\s*\", \"\", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # 2) Insert spaces on lowercase → Uppercase transitions\n",
    "    s = re.sub(r\"(?<=[a-záéíóöőúüű])(?=[A-ZÁÉÍÓÖŐÚÜŰ])\", \" \", s)\n",
    "\n",
    "    # 3) Normalize spacing around delimiters , & / +\n",
    "    s = re.sub(r\"\\s*([,&/+])\\s*\", r\" \\1 \", s)\n",
    "\n",
    "    # 4) Normalize 'and'\n",
    "    s = re.sub(r\"\\s*(and)\\s*\", r\" and \", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # 5) Collapse multiple spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # 6) Split on multiple-author delimiters\n",
    "    parts = re.split(r\"\\s*(?:,|&|and|/|\\+)\\s*\", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # 7) Normalize each author name\n",
    "    authors = []\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if part:\n",
    "            part = re.sub(r\"\\s+\", \" \", part)\n",
    "            authors.append(part.title())\n",
    "\n",
    "    # 8) Return as Google-Sheets-friendly string\n",
    "    return \"; \".join(authors) if authors else pd.NA\n",
    "\n",
    "\n",
    "# Apply cleaning\n",
    "df[\"author_normalized\"] = df[\"author\"].apply(clean_and_normalize_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bc06823d-cfcf-4e4e-9fd8-881ec7fa9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_normalize_narrators(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return pd.NA\n",
    "\n",
    "    s = str(text)\n",
    "\n",
    "    # 1) Remove constant prefix \"Narratedby:\"\n",
    "    s = re.sub(r\"^\\s*Narratedby:\\s*\", \"\", flags=re.IGNORECASE, string=s)\n",
    "\n",
    "    # 2) Insert spaces on lowercase → Uppercase transitions\n",
    "    s = re.sub(r\"(?<=[a-záéíóöőúüű])(?=[A-ZÁÉÍÓÖŐÚÜŰ])\", \" \", s)\n",
    "\n",
    "    # 3) Normalize spacing around delimiters , & / +\n",
    "    s = re.sub(r\"\\s*([,&/+])\\s*\", r\" \\1 \", s)\n",
    "\n",
    "    # 4) Normalize 'and'\n",
    "    s = re.sub(r\"\\s*(and)\\s*\", r\" and \", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # 5) Collapse multiple spaces and trim\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # 6) Split on common multi-narrator delimiters\n",
    "    parts = re.split(r\"\\s*(?:,|&|and|/|\\+)\\s*\", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # 7) Normalize each narrator name\n",
    "    narrators = []\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if part:\n",
    "            part = re.sub(r\"\\s+\", \" \", part)\n",
    "            narrators.append(part.title())\n",
    "\n",
    "    # 8) Return as a normalized list string (Google Sheets safe)\n",
    "    return \"; \".join(narrators) if narrators else pd.NA\n",
    "\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"narrator_normalized\"] = df[\"narrator\"].apply(clean_and_normalize_narrators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e7c19d41-23bb-4a15-ba33-e8afd3b8dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time_to_minutes(text):\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "\n",
    "    s = str(text).lower().strip()\n",
    "\n",
    "    # Extract hours & minutes using regex\n",
    "    # Handles formats like:\n",
    "    # \"10 hrs and 5 mins\", \"10h 5m\", \"10 hours 5 minutes\", \"10:05\"\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "\n",
    "    # Case 1: HH:MM format\n",
    "    time_match = re.search(r\"(\\d+)\\s*:\\s*(\\d+)\", s)\n",
    "    if time_match:\n",
    "        hours = int(time_match.group(1))\n",
    "        minutes = int(time_match.group(2))\n",
    "    else:\n",
    "        # Case 2: Text-based format\n",
    "        h_match = re.search(r\"(\\d+)\\s*(hour|hours|hr|hrs|h)\", s)\n",
    "        m_match = re.search(r\"(\\d+)\\s*(minute|minutes|min|mins|m)\", s)\n",
    "\n",
    "        if h_match:\n",
    "            hours = int(h_match.group(1))\n",
    "\n",
    "        if m_match:\n",
    "            minutes = int(m_match.group(1))\n",
    "\n",
    "        # Case 3: Only a single number (assume minutes)\n",
    "        if not h_match and not m_match:\n",
    "            num_match = re.search(r\"\\d+\", s)\n",
    "            if num_match:\n",
    "                minutes = int(num_match.group())\n",
    "\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    return total_minutes\n",
    "\n",
    "\n",
    "# --- Convert time → total_minutes ---\n",
    "df[\"total_minutes\"] = df[\"time\"].apply(parse_time_to_minutes)\n",
    "\n",
    "# --- VALIDATION ---\n",
    "\n",
    "# 1) Detect zero or negative values\n",
    "df[\"time_invalid_zero_negative\"] = df[\"total_minutes\"] <= 0\n",
    "\n",
    "# 2) Detect extreme values (customizable thresholds)\n",
    "MIN_VALID_MINUTES = 5       # below this is likely an error\n",
    "MAX_VALID_MINUTES = 3000    # 50+ hours is very unusual\n",
    "\n",
    "df[\"time_invalid_extreme\"] = (\n",
    "    (df[\"total_minutes\"] < MIN_VALID_MINUTES) |\n",
    "    (df[\"total_minutes\"] > MAX_VALID_MINUTES)\n",
    ")\n",
    "\n",
    "# 3) Combined validation flag\n",
    "df[\"time_validation_flag\"] = np.where(\n",
    "    df[\"total_minutes\"].isna(), \"missing\",\n",
    "    np.where(\n",
    "        df[\"time_invalid_zero_negative\"], \"zero_or_negative\",\n",
    "        np.where(\n",
    "            df[\"time_invalid_extreme\"], \"extreme_value\",\n",
    "            \"valid\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- ✅ DELETE ROWS WITH EXTREME VALUES ONLY ---\n",
    "df = df[df[\"time_validation_flag\"] != \"extreme_value\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6e2096df-6a25-4d85-940d-414020d71937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"language_normalized\"] = (\n",
    "    df[\"language\"]\n",
    "    .astype(str)\n",
    "    .str.strip()          # Remove leading/trailing spaces\n",
    "    .str.lower()          # Make everything lowercase first\n",
    "    .str.capitalize()    # First letter uppercase, rest lowercase\n",
    ")\n",
    "\n",
    "# Restore NaN where original values were missing\n",
    "df.loc[df[\"language\"].isna(), \"language_normalized\"] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "92a0e9f5-408f-43f1-bafa-afaf743ce441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stars(value):\n",
    "    \"\"\"\n",
    "    Parse strings like:\n",
    "      \"4.5 out of 5 stars (3 ratings)\"\n",
    "      \"4.5 out of 5 stars 3 ratings\"\n",
    "      \"Not rated yet\"\n",
    "    into: star_rating, max_rating, rating_count\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return pd.Series([np.nan, np.nan, np.nan])\n",
    "    \n",
    "    s = str(value).strip()\n",
    "\n",
    "    # Handle \"Not rated yet\"\n",
    "    if s.lower() == \"not rated yet\":\n",
    "        return pd.Series([np.nan, np.nan, np.nan])\n",
    "\n",
    "    # Default NaNs\n",
    "    star_rating = np.nan\n",
    "    max_rating = np.nan\n",
    "    rating_count = np.nan\n",
    "\n",
    "    # 1) Parse \"X out of Y\"\n",
    "    m = re.search(r'(\\d+(?:\\.\\d+)?)\\s*out of\\s*(\\d+(?:\\.\\d+)?)', s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        star_rating = float(m.group(1))\n",
    "        max_rating = float(m.group(2))\n",
    "\n",
    "    # 2) Parse rating count: \"N rating(s)\"\n",
    "    m_count = re.search(r'(\\d[\\d,]*)\\s*(rating|ratings)', s, flags=re.IGNORECASE)\n",
    "    if m_count:\n",
    "        rating_count = int(m_count.group(1).replace(\",\", \"\"))\n",
    "\n",
    "    return pd.Series([star_rating, max_rating, rating_count])\n",
    "\n",
    "\n",
    "# --- Apply parsing to create new columns ---\n",
    "\n",
    "df[[\"star_rating\", \"max_rating\", \"rating_count\"]] = df[\"stars\"].apply(parse_stars)\n",
    "\n",
    "\n",
    "# --- VALIDATION ---\n",
    "\n",
    "# 1) Validate star_rating in [0, max_rating]\n",
    "mask_valid_bounds = (\n",
    "    df[\"star_rating\"].notna() &\n",
    "    df[\"max_rating\"].notna()\n",
    ")\n",
    "\n",
    "df[\"star_rating_valid\"] = True  # default True for missing\n",
    "df.loc[mask_valid_bounds, \"star_rating_valid\"] = (\n",
    "    (df.loc[mask_valid_bounds, \"star_rating\"] >= 0) &\n",
    "    (df.loc[mask_valid_bounds, \"star_rating\"] <= df.loc[mask_valid_bounds, \"max_rating\"])\n",
    ")\n",
    "\n",
    "# 2) Validate rating_count is non-negative integer\n",
    "# (we parsed as int, so just check non-negative; NaN allowed)\n",
    "df[\"rating_count_valid\"] = (\n",
    "    df[\"rating_count\"].isna() | (df[\"rating_count\"] >= 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2058f983-7f26-434c-b793-9fbacc09b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any non-integer prices? True\n",
      "718\n",
      "Count of price outliers: 718\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Clean and parse price column (with \"Free\" support) ---\n",
    "\n",
    "def parse_price(value):\n",
    "    \"\"\"\n",
    "    Clean price strings and convert to float.\n",
    "    Handles:\n",
    "      - \"$9.99\", \"€ 1,234.50\", \"9 999 Ft\"\n",
    "      - \"Free\" -> 0.0\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "\n",
    "    s = str(value).strip().lower()\n",
    "\n",
    "    # Handle \"Free\"\n",
    "    if s == \"free\":\n",
    "        return 0.0\n",
    "\n",
    "    # Remove currency symbols and any non-digit/.,- characters\n",
    "    s = re.sub(r\"[^0-9\\.,-]\", \"\", s)\n",
    "\n",
    "    if s == \"\":\n",
    "        return np.nan\n",
    "\n",
    "    # If both comma and dot present -> assume comma is thousand sep\n",
    "    if \",\" in s and \".\" in s:\n",
    "        s = s.replace(\",\", \"\")\n",
    "    else:\n",
    "        # If only comma present -> assume decimal comma\n",
    "        if \",\" in s and \".\" not in s:\n",
    "            s = s.replace(\",\", \".\")\n",
    "\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# --- Apply parsing ---\n",
    "df[\"price_numeric\"] = df[\"price\"].apply(parse_price)\n",
    "\n",
    "# --- 2) Flag whether the item is free ---\n",
    "df[\"price_is_free\"] = df[\"price\"].astype(str).str.strip().str.lower().eq(\"free\")\n",
    "\n",
    "# --- 3) Investigate integer vs non-integer prices ---\n",
    "\n",
    "df[\"price_is_integer\"] = df[\"price_numeric\"].notna() & (\n",
    "    (df[\"price_numeric\"] % 1).abs() < 1e-9\n",
    ")\n",
    "\n",
    "any_non_integer = (~df[\"price_is_integer\"] & df[\"price_numeric\"].notna()).any()\n",
    "print(\"Any non-integer prices?\", any_non_integer)\n",
    "\n",
    "# --- 4) Consistent display format (two decimals) ---\n",
    "\n",
    "df[\"price_display\"] = df[\"price_numeric\"].apply(\n",
    "    lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\"\n",
    ")\n",
    "\n",
    "# --- 5) Flag extreme outliers (IQR method) ---\n",
    "\n",
    "valid_prices = df[\"price_numeric\"].dropna()\n",
    "\n",
    "if len(valid_prices) > 0:\n",
    "    Q1 = valid_prices.quantile(0.25)\n",
    "    Q3 = valid_prices.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    df[\"price_outlier\"] = df[\"price_numeric\"].notna() & (\n",
    "        (df[\"price_numeric\"] < lower_bound) |\n",
    "        (df[\"price_numeric\"] > upper_bound)\n",
    "    )\n",
    "else:\n",
    "    df[\"price_outlier\"] = False\n",
    "\n",
    "print(len(df[df[\"price_outlier\"] == True]))\n",
    "\n",
    "print(\"Count of price outliers: \" + str(len(df[df[\"price_outlier\"] == True])))\n",
    "\n",
    "# --- 6) Flag negative prices ---\n",
    "\n",
    "df[\"price_negative\"] = (\n",
    "    df[\"price_numeric\"].notna() &\n",
    "    (df[\"price_numeric\"] < 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f211b115-eddb-4728-8322-dbd2a05300bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Sheet 'Audable Web Scraped' → 'Working Dataset' updated successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cols = [\n",
    "    \"price\",\n",
    "    \"price_numeric\",\n",
    "    \"price_is_free\",\n",
    "    \"price_is_integer\",\n",
    "    \"price_display\",\n",
    "    \"price_outlier\",\n",
    "    \"price_negative\",\n",
    "]\n",
    "\n",
    "# Make a copy of just the columns you want to upload\n",
    "df_gsheet = df[cols].copy()\n",
    "\n",
    "# Replace NaN / inf with empty string (JSON-safe, Google Sheets-safe)\n",
    "df_gsheet = df_gsheet.replace([np.nan, np.inf, -np.inf], \"\")\n",
    "\n",
    "# Now upload this cleaned version\n",
    "update_google_sheet(\n",
    "    df_gsheet,\n",
    "    sheet_name=\"Audable Web Scraped\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "658d5fcb-4c4e-4cd5-a2b3-4fdb68c33619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.DataFrame()\n",
    "df_cleaned = df[[\"name_normalized\", \"author_normalized\", \"narrator_normalized\",\n",
    "                 \"total_minutes\", \"language_normalized\", \"star_rating\",\n",
    "                 \"max_rating\", \"rating_count\", \"price_numeric\"]]\n",
    "\n",
    "df_cleaned.columns = [\n",
    "    \"Name\",\n",
    "    \"Author\",\n",
    "    \"Narrator\",\n",
    "    \"TotalMinutes\",\n",
    "    \"Language\",\n",
    "    \"StarRating\",\n",
    "    \"MaxRating\",\n",
    "    \"RatingCount\",\n",
    "    \"Price\"\n",
    "]\n",
    "\n",
    "# Replace NaN / inf with empty string (JSON-safe, Google Sheets-safe)\n",
    "df_cleaned = df_cleaned.replace([np.nan, np.inf, -np.inf], \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1acd0f07-c0c7-44ba-b3f8-a3c06d19f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Sheet 'Audable Web Scraped' → 'Working Dataset' updated successfully!\n"
     ]
    }
   ],
   "source": [
    "update_google_sheet(df_cleaned, sheet_name=\"Audable Web Scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7638dfc3-470a-47db-90df-ce99e971c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"audible_cleaned.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
